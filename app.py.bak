from flask import Flask, render_template, request, jsonify, Response
from flask_cors import CORS
from flask_sqlalchemy import SQLAlchemy
from datetime import datetime
import requests
import json
import re
import hashlib
import os
import numpy as np

app = Flask(__name__)
CORS(app)

# Database configuration
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///joeybot.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
db = SQLAlchemy(app)

# Database Models
class Conversation(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    title = db.Column(db.String(100), nullable=False)
    summary = db.Column(db.Text)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    last_updated = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    # Rolling summary fields for auto-summarization
    rolling_summary = db.Column(db.Text, default='')
    messages_summarized = db.Column(db.Integer, default=0)
    last_summary_at = db.Column(db.DateTime, nullable=True)
    messages = db.relationship('Message', backref='conversation', lazy=True, cascade='all, delete-orphan')

class Message(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    conversation_id = db.Column(db.Integer, db.ForeignKey('conversation.id'), nullable=True)
    role = db.Column(db.String(20), nullable=False)  # 'user' or 'assistant'
    content = db.Column(db.Text, nullable=False)
    timestamp = db.Column(db.DateTime, default=datetime.utcnow)

class MemoryMarker(db.Model):
    """Represents a compression checkpoint where older messages are summarized"""
    id = db.Column(db.Integer, primary_key=True)
    conversation_id = db.Column(db.Integer, db.ForeignKey('conversation.id'), nullable=False)
    summary_text = db.Column(db.Text, nullable=False)
    covers_before = db.Column(db.DateTime, nullable=False)  # Messages before this timestamp are summarized
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    messages_compressed = db.Column(db.Integer, default=0)
    tokens_saved = db.Column(db.Integer, default=0)

    conversation = db.relationship('Conversation', backref=db.backref('memory_markers', lazy=True, cascade='all, delete-orphan'))

class UserProfile(db.Model):
    """Stores persistent user details for AI context"""
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), default='')
    details = db.Column(db.Text, default='')  # Max 125 words enforced in frontend
    updated_at = db.Column(db.DateTime, default=datetime.utcnow)

class TokenUsage(db.Model):
    """Tracks token usage per AI response"""
    id = db.Column(db.Integer, primary_key=True)
    conversation_id = db.Column(db.Integer, db.ForeignKey('conversation.id'), nullable=True)
    tokens_output = db.Column(db.Integer, default=0)  # Tokens generated by AI
    tokens_per_second = db.Column(db.Float, default=0)  # Speed of generation
    duration_ms = db.Column(db.Integer, default=0)  # Time taken in milliseconds
    timestamp = db.Column(db.DateTime, default=datetime.utcnow)

# Initialize database
with app.app_context():
    db.create_all()

# ============================================
# SEMANTIC MEMORY (TIER 2) CONFIGURATION
# ============================================
SEMANTIC_MEMORY_ENABLED = True
EMBEDDING_MODEL = "nomic-embed-text"
SEMANTIC_RESULTS_COUNT = 3
SIMILARITY_THRESHOLD = 0.92
VECTOR_STORE_PATH = os.path.join(app.instance_path, 'semantic_memory.json')

def load_vector_store():
    """Load the vector store from JSON file."""
    if os.path.exists(VECTOR_STORE_PATH):
        try:
            with open(VECTOR_STORE_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"Error loading vector store: {e}")
    return {"facts": []}

def save_vector_store(store):
    """Save the vector store to JSON file."""
    try:
        os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)
        with open(VECTOR_STORE_PATH, 'w', encoding='utf-8') as f:
            json.dump(store, f, indent=2)
    except Exception as e:
        print(f"Error saving vector store: {e}")

def get_embedding(text):
    """Get embedding vector from nomic-embed-text via Ollama."""
    try:
        response = requests.post(
            "http://localhost:11434/api/embeddings",
            json={"model": EMBEDDING_MODEL, "prompt": text},
            timeout=30
        )
        if response.status_code == 200:
            return response.json().get('embedding')
    except Exception as e:
        print(f"Embedding error: {e}")
    return None

def cosine_similarity(vec1, vec2):
    """Compute cosine similarity between two vectors."""
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return dot_product / (norm1 * norm2)

def normalize_fact(fact_text):
    """Normalize fact text for deduplication."""
    return ' '.join(fact_text.lower().strip().split())

def compute_fact_hash(fact_text):
    """Compute SHA256 hash of normalized fact."""
    return hashlib.sha256(normalize_fact(fact_text).encode()).hexdigest()

def is_duplicate_fact(store, new_embedding, threshold=0.92):
    """Check if a similar fact already exists using cosine similarity."""
    for fact_entry in store.get("facts", []):
        existing_embedding = fact_entry.get("embedding")
        if existing_embedding:
            similarity = cosine_similarity(new_embedding, existing_embedding)
            if similarity > threshold:
                return True
    return False

# Fact extraction prompt
FACT_EXTRACTION_PROMPT = """Extract permanent, reusable facts from this conversation for long-term memory.

MESSAGES:
{messages_text}

Extract ONLY:
- User preferences (tools, languages, frameworks they prefer)
- Project details (what they're building, tech stack decisions)
- Technical decisions and their reasoning
- Background info stated by the user
- Important milestones or achievements

DO NOT extract:
- Temporary states or current activities
- Questions or uncertainties
- Generic statements
- Things already in their profile

Format: JSON array of standalone sentences.
Example: ["User prefers Python for backend.", "Project uses SQLite database."]
Return [] if no permanent facts found.

Facts:"""

def extract_semantic_facts(messages_text):
    """Extract permanent facts from messages using Gemma."""
    prompt = FACT_EXTRACTION_PROMPT.format(messages_text=messages_text)
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": "gemma3:4b", "prompt": prompt, "stream": False},
            timeout=60
        )
        if response.status_code == 200:
            text = response.json().get('response', '').strip()
            # Parse JSON array from response
            match = re.search(r'\[.*\]', text, re.DOTALL)
            if match:
                return json.loads(match.group())
    except Exception as e:
        print(f"Fact extraction error: {e}")
    return []

def store_facts(facts, conversation_id, message_ids):
    """Store extracted facts with deduplication."""
    if not facts:
        return 0

    store = load_vector_store()
    stored = 0

    for fact in facts:
        if not fact or len(fact) < 10:
            continue

        embedding = get_embedding(fact)
        if not embedding:
            continue

        if is_duplicate_fact(store, embedding, SIMILARITY_THRESHOLD):
            continue

        fact_id = compute_fact_hash(fact)
        store["facts"].append({
            "id": fact_id,
            "text": fact,
            "embedding": embedding,
            "conversation_id": conversation_id or 0,
            "timestamp": datetime.utcnow().isoformat(),
            "message_ids": message_ids
        })
        stored += 1

    if stored > 0:
        save_vector_store(store)

    return stored

def process_semantic_memory(conversation_id, messages_text, message_ids):
    """Main entry point: extract and store facts after summarization."""
    if not SEMANTIC_MEMORY_ENABLED:
        return 0
    facts = extract_semantic_facts(messages_text)
    if facts:
        return store_facts(facts, conversation_id, message_ids)
    return 0

def estimate_tokens(text):
    """Rough token estimate: ~4 characters per token for English text."""
    if not text:
        return 0
    return len(text) // 4

def get_user_profile_context():
    """Build user profile prefix for AI context"""
    profile = UserProfile.query.first()
    if not profile or (not profile.name and not profile.details):
        return ''

    parts = ['[User Profile]']
    if profile.name:
        parts.append(f'Name: {profile.name}.')
    if profile.details:
        parts.append(f'About the user: {profile.details}')
    return ' '.join(parts) + '\n\n'

def get_long_term_memory(query_text=None, n_results=3):
    """Retrieve semantically relevant facts from vector store."""
    if not query_text or not SEMANTIC_MEMORY_ENABLED:
        return ''

    try:
        store = load_vector_store()
        facts_list = store.get("facts", [])

        if not facts_list:
            return ''

        query_embedding = get_embedding(query_text)
        if not query_embedding:
            return ''

        # Calculate similarity scores for all facts
        scored_facts = []
        for fact_entry in facts_list:
            fact_embedding = fact_entry.get("embedding")
            if fact_embedding:
                similarity = cosine_similarity(query_embedding, fact_embedding)
                scored_facts.append((similarity, fact_entry["text"]))

        # Sort by similarity (descending) and take top n_results
        scored_facts.sort(key=lambda x: x[0], reverse=True)
        top_facts = scored_facts[:n_results]

        if not top_facts:
            return ''

        # Format as bullet points
        return '\n'.join(f"- {fact}" for _, fact in top_facts)

    except Exception as e:
        print(f"Memory retrieval error: {e}")
        return ''

def build_context_v2(conversation_id, new_message, mode='normal', history=None):
    """
    Build context using new tiered memory:
    1. Tonality prefix (if mode != normal)
    2. User Profile
    3. Long-term memory (placeholder)
    4. Rolling conversation summary
    5. Last 8 raw messages
    6. Current user message
    """
    context_parts = []

    # 1. Tonality prefix
    if mode == 'concise':
        context_parts.append("[Mode: Be concise. Answer in 2-3 sentences maximum.]")
    elif mode == 'logic':
        context_parts.append("[Mode: Think step-by-step. Show your reasoning process.]")

    # 2. User Profile
    user_context = get_user_profile_context()
    if user_context:
        context_parts.append(f"[User Profile]: {user_context}")

    # 3. Long-term memory (semantic retrieval)
    ltm = get_long_term_memory(query_text=new_message, n_results=SEMANTIC_RESULTS_COUNT)
    if ltm:
        context_parts.append(f"[Relevant Memory]:\n{ltm}")

    # 4 & 5: Rolling summary and recent messages
    if conversation_id:
        convo = Conversation.query.get(conversation_id)

        # Add rolling summary
        if convo and convo.rolling_summary:
            context_parts.append(f"[Conversation Summary]: {convo.rolling_summary}")

        # Get last 8 messages
        recent_msgs = Message.query.filter_by(conversation_id=conversation_id)\
            .order_by(Message.timestamp.desc()).limit(8).all()
        recent_msgs.reverse()

        for msg in recent_msgs:
            prefix = "User" if msg.role == 'user' else "Assistant"
            context_parts.append(f"{prefix}: {msg.content}")
    elif history:
        recent = history[-8:] if len(history) > 8 else history
        for msg in recent:
            prefix = "User" if msg['role'] == 'user' else "Assistant"
            context_parts.append(f"{prefix}: {msg['content']}")

    # 6. Current message
    context_parts.append(f"User: {new_message}")
    context_parts.append("Assistant:")

    return "\n".join(context_parts)

# Rolling summary prompt for auto-summarization
ROLLING_SUMMARY_PROMPT = """You are updating a conversation summary. Given the existing summary and new messages, create an updated summary.

EXISTING SUMMARY:
{existing_summary}

NEW MESSAGES TO ADD:
{new_messages}

Create an updated summary that:
- Preserves key information from existing summary
- Adds important new topics and decisions
- Stays concise (150-300 words)
- Prioritizes information useful for continuing the conversation

Updated summary:"""

def auto_summarize_conversation(conversation_id):
    """Auto-update rolling summary when 10+ new messages exist."""
    convo = Conversation.query.get(conversation_id)
    if not convo:
        return False

    total_messages = Message.query.filter_by(conversation_id=conversation_id).count()
    messages_in_summary = convo.messages_summarized or 0
    unsummarized_count = total_messages - messages_in_summary

    if unsummarized_count < 8:
        return False

    # Get messages to summarize (excluding last 8 for raw context)
    all_msgs = Message.query.filter_by(conversation_id=conversation_id)\
        .order_by(Message.timestamp.asc()).all()

    messages_to_summarize = all_msgs[messages_in_summary:total_messages - 8]

    if len(messages_to_summarize) < 5:
        return False

    new_messages_text = "\n".join([
        f"{'User' if m.role == 'user' else 'Assistant'}: {m.content}"
        for m in messages_to_summarize
    ])

    existing_summary = convo.rolling_summary or "No previous summary."

    prompt = ROLLING_SUMMARY_PROMPT.format(
        existing_summary=existing_summary,
        new_messages=new_messages_text
    )

    try:
        response = requests.post('http://localhost:11434/api/generate',
            json={"model": "gemma3:4b", "prompt": prompt, "stream": False},
            timeout=60)

        result = response.json()
        new_summary = result.get('response', '').strip()

        if new_summary:
            convo.rolling_summary = new_summary
            convo.messages_summarized = total_messages - 8
            convo.last_summary_at = datetime.utcnow()
            db.session.commit()

            # Trigger semantic fact extraction from summarized messages
            try:
                message_ids = [m.id for m in messages_to_summarize]
                facts_stored = process_semantic_memory(
                    conversation_id,
                    new_messages_text,
                    message_ids
                )
                if facts_stored > 0:
                    print(f"Stored {facts_stored} semantic facts from conversation {conversation_id}")
            except Exception as e:
                print(f"Semantic memory extraction error: {e}")

            return True
    except Exception as e:
        print(f"Auto-summarization error: {e}")

    return False

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/user-profile', methods=['GET'])
def get_user_profile():
    """Get user profile details"""
    profile = UserProfile.query.first()
    if not profile:
        return jsonify({'name': '', 'details': ''})
    return jsonify({'name': profile.name, 'details': profile.details})

@app.route('/user-profile', methods=['POST'])
def save_user_profile():
    """Save user profile details"""
    data = request.json
    profile = UserProfile.query.first()
    if not profile:
        profile = UserProfile()
        db.session.add(profile)
    profile.name = data.get('name', '')[:100]  # Limit name length
    profile.details = data.get('details', '')
    profile.updated_at = datetime.utcnow()
    db.session.commit()
    return jsonify({'success': True})

@app.route('/token-usage', methods=['POST'])
def log_token_usage():
    """Log token usage after AI response"""
    data = request.json
    usage = TokenUsage(
        conversation_id=data.get('conversation_id'),
        tokens_output=data.get('tokens_output', 0),
        tokens_per_second=data.get('tokens_per_second', 0),
        duration_ms=data.get('duration_ms', 0)
    )
    db.session.add(usage)
    db.session.commit()
    return jsonify({'success': True, 'id': usage.id})

@app.route('/usage-stats', methods=['GET'])
def get_usage_stats():
    """Get global usage statistics"""
    from sqlalchemy import func

    # Total tokens all-time
    total_tokens = db.session.query(func.sum(TokenUsage.tokens_output)).scalar() or 0

    # Total conversations (saved)
    total_conversations = Conversation.query.count()

    # Average tokens per chat (only for saved conversations with usage data)
    avg_result = db.session.query(
        func.avg(TokenUsage.tokens_output)
    ).filter(TokenUsage.conversation_id.isnot(None)).scalar()
    avg_per_chat = round(avg_result or 0)

    # Average tokens per second
    avg_speed = db.session.query(
        func.avg(TokenUsage.tokens_per_second)
    ).filter(TokenUsage.tokens_per_second > 0).scalar()
    avg_tokens_per_second = round(avg_speed or 0, 1)

    return jsonify({
        'total_tokens': total_tokens,
        'total_conversations': total_conversations,
        'avg_per_chat': avg_per_chat,
        'avg_tokens_per_second': avg_tokens_per_second
    })

@app.route('/semantic-memory-stats', methods=['GET'])
def get_semantic_memory_stats():
    """Get semantic memory (Tier 2) statistics."""
    store = load_vector_store()
    facts = store.get("facts", [])

    # Count facts by conversation
    conversation_counts = {}
    for fact in facts:
        conv_id = fact.get("conversation_id", 0)
        conversation_counts[conv_id] = conversation_counts.get(conv_id, 0) + 1

    return jsonify({
        'enabled': SEMANTIC_MEMORY_ENABLED,
        'total_facts': len(facts),
        'conversations_with_facts': len(conversation_counts),
        'embedding_model': EMBEDDING_MODEL,
        'storage_path': VECTOR_STORE_PATH
    })

@app.route('/chat', methods=['POST'])
def chat():
    data = request.json
    user_message = data.get('message')
    mode = data.get('mode', 'normal')
    conversation_id = data.get('conversation_id')  # null for unsaved
    history = data.get('history', [])  # in-memory history for unsaved chats

    # Build context using new unified context builder
    # Mode handling is now inside build_context_v2
    context = build_context_v2(conversation_id, user_message, mode, history)
    prompt = context

    def generate():
        try:
            response = requests.post('http://localhost:11434/api/generate',
                json={
                    "model": "gemma3:4b",
                    "prompt": prompt,
                    "stream": True
                },
                stream=True)

            for line in response.iter_lines():
                if line:
                    json_response = json.loads(line)

                    if 'response' in json_response:
                        token = json_response['response']
                        yield f"data: {json.dumps({'token': token})}\n\n"
                    if json_response.get('done', False):
                        yield f"data: {json.dumps({'done': True})}\n\n"
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    return Response(generate(), mimetype='text/event-stream')

@app.route('/conversations', methods=['GET'])
def get_conversations():
    """Get all saved conversations"""
    convos = Conversation.query.order_by(Conversation.last_updated.desc()).all()
    return jsonify([{
        'id': c.id,
        'title': c.title,
        'summary': c.summary,
        'last_updated': c.last_updated.isoformat()
    } for c in convos])

@app.route('/conversation/<int:id>', methods=['GET'])
def get_conversation(id):
    """Load a conversation with its messages"""
    convo = Conversation.query.get_or_404(id)
    messages = [{'role': m.role, 'content': m.content} for m in convo.messages]
    return jsonify({
        'id': convo.id,
        'title': convo.title,
        'summary': convo.summary,
        'messages': messages
    })

@app.route('/conversation/<int:id>', methods=['DELETE'])
def delete_conversation(id):
    """Delete a conversation"""
    convo = Conversation.query.get_or_404(id)
    db.session.delete(convo)
    db.session.commit()
    return jsonify({'success': True})

@app.route('/save-chat', methods=['POST'])
def save_chat():
    """Save current conversation with AI-generated title/summary"""
    data = request.json
    messages = data.get('messages', [])

    if not messages:
        return jsonify({'error': 'No messages to save'}), 400

    # Build prompt for title/summary generation
    history = "\n".join([f"{m['role']}: {m['content']}" for m in messages])
    prompt = f"""Conversation:
{history}
 
Generate a JSON response with:
- "title": A descriptive 5-7 word title for this conversation
- "summary": A 2-5 sentence summary of the main topics discussed

Respond ONLY with valid JSON, no other text. Example format:
{{"title": "Example Title Here", "summary": "Example summary here."}}"""

    try:
        # Call Ollama (non-streaming)
        response = requests.post('http://localhost:11434/api/generate',
            json={"model": "gemma3:4b", "prompt": prompt, "stream": False})

        result = response.json()
        response_text = result.get('response', '')

        # Try to parse JSON from response
        try:
            # Find JSON in response
            json_match = re.search(r'\{[^}]+\}', response_text, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group())
                title = parsed.get('title', 'Untitled Chat')[:100]
                summary = parsed.get('summary', '')
            else:
                title = 'Untitled Chat'
                summary = ''
        except json.JSONDecodeError:
            title = 'Untitled Chat'
            summary = ''

        # Create conversation record
        convo = Conversation(title=title, summary=summary)
        db.session.add(convo)
        db.session.flush()  # Get the ID

        # Save all messages
        for msg in messages:
            m = Message(
                conversation_id=convo.id,
                role=msg['role'],
                content=msg['content']
            )
            db.session.add(m)

        db.session.commit()

        return jsonify({
            'id': convo.id,
            'title': title,
            'summary': summary
        })

    except Exception as e:
        print(f"Error saving chat: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/message', methods=['POST'])
def save_message():
    """Save a single message to a conversation"""
    data = request.json
    conversation_id = data.get('conversation_id')

    if not conversation_id:
        return jsonify({'error': 'conversation_id required'}), 400

    # Update conversation's last_updated
    convo = Conversation.query.get(conversation_id)
    if convo:
        convo.last_updated = datetime.utcnow()

    msg = Message(
        conversation_id=conversation_id,
        role=data['role'],
        content=data['content']
    )
    db.session.add(msg)
    db.session.commit()

    # Trigger auto-summarization after assistant messages
    summarized = False
    if data['role'] == 'assistant':
        summarized = auto_summarize_conversation(conversation_id)

    return jsonify({'id': msg.id, 'summarized': summarized})

@app.route('/memory-stats/<int:conversation_id>', methods=['GET'])
def get_memory_stats(conversation_id):
    """Get simplified memory statistics for a conversation."""
    convo = Conversation.query.get(conversation_id)
    if not convo:
        return jsonify({'error': 'Not found'}), 404

    total = Message.query.filter_by(conversation_id=conversation_id).count()

    return jsonify({
        'total_messages': total,
        'messages_summarized': convo.messages_summarized or 0,
        'has_summary': bool(convo.rolling_summary),
        'last_summary_at': convo.last_summary_at.isoformat() if convo.last_summary_at else None
    })

if __name__ == '__main__':
    app.run(debug=True, port=5000)
